{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zstandard\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import psutil\n",
    "\n",
    "subreddit_list = ['AskReddit']#['politics', 'memes', 'teenagers', 'AmItheAsshole', 'AskReddit']\n",
    "comment_fields_list = ['link_id', 'id', 'parent_id', 'author', 'author_fullname', 'permalink',\n",
    "                       'body', 'total_awards_received', 'score', 'subreddit', 'removal_reason']\n",
    "submission_fields_list = ['id', 'num_comments', 'title', 'author', 'author_fullname', 'permalink', \n",
    "                          'selftext', 'total_awards_received', 'score', 'subreddit', 'removal_reason']\n",
    "data_path = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RC_2019-05.zst',\n",
       " 'RC_2019-04.csv.gz',\n",
       " 'RS_2019-06.zst',\n",
       " 'RC_2019-02.zst',\n",
       " 'RC_2018-11.zst',\n",
       " 'RS_2019-01.zst',\n",
       " 'RC_2019-10.csv.gz',\n",
       " 'RC_2018-12.csv.gz',\n",
       " 'RC_2019-03.zst',\n",
       " 'RC_2019-09.csv.gz',\n",
       " 'RS_2019-05.zst',\n",
       " 'RC_2018-10.csv.gz',\n",
       " 'RS_2019-10.zst',\n",
       " 'RC_2019-05.csv.gz',\n",
       " 'RC_2018-11.csv.gz',\n",
       " 'RC_2019-11.csv.gz',\n",
       " 'RC_2019-07.zst',\n",
       " 'RC_2019-09.zst',\n",
       " 'RS_2019-04.zst',\n",
       " 'RC_2019-10.zst',\n",
       " 'RS_2019-08.zst',\n",
       " 'RC_2019-06.csv.gz',\n",
       " 'RC_2019-06.zst',\n",
       " 'RC_2019-01.zst',\n",
       " 'RS_2019-12.zst',\n",
       " 'RS_2018-11.zst',\n",
       " 'RC_2019-08.zst',\n",
       " 'RS_2019-07.zst',\n",
       " 'RC_2018-12.zst',\n",
       " 'RC_2019-08.csv.gz',\n",
       " 'RC_2019-12.csv.gz',\n",
       " 'RC_2019-01.csv.gz',\n",
       " 'RC_2019-02.csv.gz',\n",
       " 'RS_2019-03.zst',\n",
       " 'RC_2019-11.zst',\n",
       " 'RC_2018-10.zst',\n",
       " 'RS_2019-09.zst',\n",
       " 'RS_2019-02.zst',\n",
       " 'RC_2019-12.zst',\n",
       " 'RC_2019-04.zst',\n",
       " 'RS_2019-11.zst',\n",
       " 'RC_2019-07.csv.gz',\n",
       " 'RC_2019-03.csv.gz',\n",
       " 'RS_2018-12.zst']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc_list = []\n",
    "for f in os.listdir(data_path):\n",
    "    if (f.find('RC')>=0) & (f.find('zst')>=0) & (f.replace('zst', 'csv.gz') not in os.listdir(data_path)):\n",
    "        rc_list.append(os.path.join(data_path, f))\n",
    "rc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fl = os.path.join(data_path, 'RS_2018-11.zst')\n",
    "tail = ''\n",
    "j = 0\n",
    "result = []\n",
    "with open(fl, 'rb') as fh:\n",
    "    dctx = zstandard.ZstdDecompressor()\n",
    "    reader = dctx.stream_reader(fh)\n",
    "    while True:\n",
    "        j += 1\n",
    "        chunk = reader.read(16384 * 2)\n",
    "        if not chunk:\n",
    "            break\n",
    "        chunk = tail + chunk.decode()\n",
    "        lst = chunk.split('\\n')\n",
    "        for i in range(len(lst) - 1):\n",
    "            dct = json.loads(lst[i])\n",
    "            if (dct['subreddit'] not in subreddit_list) | (dct['author'] == '[deleted]'):\n",
    "                continue \n",
    "            print({i: dct.get(i, i) for i in submission_fields_list})\n",
    "        if j > 100:\n",
    "            break\n",
    "        tail = lst[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_fl(fl, subreddit_list, fields_list):\n",
    "    tail = ''\n",
    "    j = 0\n",
    "    result = []\n",
    "    with open(fl, 'rb') as fh:\n",
    "        dctx = zstandard.ZstdDecompressor()\n",
    "        reader = dctx.stream_reader(fh)\n",
    "        while True:\n",
    "            j += 1\n",
    "            chunk = reader.read(16384 * 2)\n",
    "            if not chunk:\n",
    "                break\n",
    "            chunk = tail + chunk.decode()\n",
    "            lst = chunk.split('\\n')\n",
    "            for i in range(len(lst) - 1):\n",
    "                dct = json.loads(lst[i])\n",
    "                if (dct['subreddit'] not in subreddit_list) | (dct['author'] == '[deleted]'):\n",
    "                    continue\n",
    "                ans = [dct.get(i, None) for i in fields_list]\n",
    "                result.append(ans)\n",
    "            if j % 1000 == 0:\n",
    "                print(\"\\rEpisode {}, len of result {}, usage of memory {}.\".format(\n",
    "                    j, len(result), psutil.virtual_memory().percent), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "            tail = lst[-1]\n",
    "            if psutil.virtual_memory().percent > 95:\n",
    "                break\n",
    "    print()\n",
    "    return result\n",
    "\n",
    "def preproc_fl_list(fl_list, subreddit_list, fields_list):\n",
    "    res = []\n",
    "    for fl in fl_list:\n",
    "        res = preproc_fl(fl, subreddit_list, fields_list)\n",
    "        df = pd.DataFrame(res, columns = fields_list)\n",
    "        if 'body' in fields_list:\n",
    "            df.body = df.body.apply(lambda x: str(x).replace(';', ','))\n",
    "        if 'selftext' in fields_list:\n",
    "            df.selftext = df.selftext.apply(lambda x: str(x).replace(';', ','))\n",
    "        print(df.shape)\n",
    "        df.to_csv(fl.replace('.zst', '.csv.gz'),\n",
    "                  compression = 'gzip',\n",
    "                  sep = ';',\n",
    "                  index = False\n",
    "                 )\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "CPU times: user 37 µs, sys: 4 µs, total: 41 µs\n",
      "Wall time: 43.6 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rc_res = preproc_fl_list(rc_list, subreddit_list, comment_fields_list)\n",
    "print(rc_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "df = pd.DataFrame(rc_res, columns = comment_fields_list)\n",
    "df.body = df.body.apply(lambda x: str(x).replace(';', ','))\n",
    "print(df.shape)\n",
    "\n",
    "df.to_csv(os.path.join(data_path, 'rc_preprocessed.csv.gz'),\n",
    "          compression = 'gzip',\n",
    "          sep = ';',\n",
    "          index = False\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/RS_2019-06.zst',\n",
       " 'data/RS_2019-01.zst',\n",
       " 'data/RS_2019-05.zst',\n",
       " 'data/RS_2019-10.zst',\n",
       " 'data/RS_2019-04.zst',\n",
       " 'data/RS_2019-08.zst',\n",
       " 'data/RS_2019-12.zst',\n",
       " 'data/RS_2018-11.zst',\n",
       " 'data/RS_2019-07.zst',\n",
       " 'data/RS_2019-03.zst',\n",
       " 'data/RS_2019-09.zst',\n",
       " 'data/RS_2019-02.zst',\n",
       " 'data/RS_2019-11.zst',\n",
       " 'data/RS_2018-12.zst']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_list = []\n",
    "for f in os.listdir(data_path):\n",
    "    #if (f.find('RS')>=0) & (f.find('zst')>=0):\n",
    "    if (f.find('RS')>=0) & (f.find('zst')>=0) & (f.replace('zst', 'csv.gz') not in os.listdir(data_path)):\n",
    "        rs_list.append(os.path.join(data_path, f))\n",
    "rs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1841000, len of result 297883, usage of memory 8.6.\n",
      "(297903, 11)\n",
      "Episode 1545000, len of result 239925, usage of memory 8.9.\n",
      "(240014, 11)\n",
      "Episode 1815000, len of result 314310, usage of memory 9.0.\n",
      "(314467, 11)\n",
      "Episode 2067000, len of result 297029, usage of memory 9.1.\n",
      "(297051, 11)\n",
      "Episode 1784000, len of result 302266, usage of memory 9.1.\n",
      "(302387, 11)\n",
      "Episode 2090000, len of result 367115, usage of memory 9.2.\n",
      "(367143, 11)\n",
      "Episode 2084000, len of result 317647, usage of memory 9.3.\n",
      "(317692, 11)\n",
      "Episode 1266000, len of result 180164, usage of memory 9.1.\n",
      "(180192, 11)\n",
      "Episode 1971000, len of result 337181, usage of memory 9.0.\n",
      "(337241, 11)\n",
      "Episode 1783000, len of result 276047, usage of memory 9.1.\n",
      "(276070, 11)\n",
      "Episode 1944000, len of result 282044, usage of memory 9.1.\n",
      "(282177, 11)\n",
      "Episode 1480000, len of result 235029, usage of memory 9.1.\n",
      "(235231, 11)\n",
      "Episode 2039000, len of result 286074, usage of memory 9.1.\n",
      "(286145, 11)\n",
      "Episode 1388000, len of result 201500, usage of memory 9.1.\n",
      "(201679, 11)\n",
      "1\n",
      "CPU times: user 2h 24min 44s, sys: 4min 1s, total: 2h 28min 45s\n",
      "Wall time: 2h 28min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rs_res = preproc_fl_list(rs_list, subreddit_list, submission_fields_list)\n",
    "print(rs_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "df = pd.DataFrame(rs_res, columns = submission_fields_list)\n",
    "df.selftext = df.selftext.apply(lambda x: str(x).replace(';', ','))\n",
    "print(df.shape)\n",
    "\n",
    "df.to_csv(os.path.join(data_path, 'rs_preprocessed.csv.gz'),\n",
    "          compression = 'gzip',\n",
    "          sep = ';',\n",
    "          index = False\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
